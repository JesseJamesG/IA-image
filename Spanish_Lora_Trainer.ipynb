{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JesseJamesG/IA-image/blob/main/Spanish_Lora_Trainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmCPmqFL6hCQ"
      },
      "source": [
        "# â­ Entrenador de Lora de Hollowstrawberry\n",
        "\n",
        "Basado en el trabajo de [Kohya_ss](https://github.com/kohya-ss/sd-scripts) y [Linaqruf](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-dreambooth.ipynb#scrollTo=-Z4w3lfFKLjr). Â¡Gracias!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJ8clWTZEu-g"
      },
      "source": [
        "### â­• Disclaimer\n",
        "The purpose of this document is to research bleeding-edge technologies in the field of machine learning.  \n",
        "Please read and follow the [Google Colab guidelines](https://research.google.com/colaboratory/faq.html) and its [Terms of Service](https://research.google.com/colaboratory/tos_v3.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPQlB4djNm3C"
      },
      "source": [
        "| |GitHub|ğŸ‡¬ğŸ‡§ English|ğŸ‡ªğŸ‡¸ Spanish|\n",
        "|:--|:-:|:-:|:-:|\n",
        "| ğŸ  **Origen** | [![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/hollowstrawberry/kohya-colab) | | |\n",
        "| ğŸ“Š **Dataset Maker** | [![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/hollowstrawberry/kohya-colab/blob/main/Dataset_Maker.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Dataset_Maker.ipynb) | [![Abrir en Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge-spanish.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Spanish_Dataset_Maker.ipynb) |\n",
        "| â­ **Lora Trainer** | [![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer.ipynb) | [![Abrir en Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge-spanish.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Spanish_Lora_Trainer.ipynb) |\n",
        "| ğŸŒŸ **XL Lora Trainer** | [![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer_XL.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer_XL.ipynb) |  |\n",
        "| ğŸŒŸ **Legacy XL Trainer** | [![GitHub](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/github.svg)](https://github.com/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer_XL_Legacy.ipynb) | [![Open in Colab](https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/assets/colab-badge.svg)](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Lora_Trainer_XL_Legacy.ipynb) |  |"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Forzamos la armonÃ­a entre versiones para quitar el error de 'torchvision::nms'\n",
        "!pip install -q torch==2.4.1 torchvision==0.19.1 xformers==0.0.28.post1 --index-url https://download.pytorch.org/whl/cu121"
      ],
      "metadata": {
        "id": "IuJw_jIcOlSt",
        "outputId": "4d89a3bf-ce4f-462f-df01-abe8ef20c9e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/7.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/7.1 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m120.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalamos las piezas de configuraciÃ³n que faltan\n",
        "!pip install -q voluptuous easygui"
      ],
      "metadata": {
        "id": "KcXVCl58PE2N",
        "outputId": "def49e5e-5a5d-4c00-be5c-53ba33ac05fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/92.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m92.7/92.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalamos la pieza que falta para que el optimizador pueda arrancar\n",
        "!pip install -q bitsandbytes==0.43.0"
      ],
      "metadata": {
        "id": "b8nW91hyQBnm",
        "outputId": "e1b44b9f-bae5-428e-efe2-def4e974c338",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m102.2/102.2 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form",
        "id": "OglZzI_ujZq-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd4d36c9-51b0-412d-f722-5a117aa367b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ’¿ Revisando archivos...\n",
            "ğŸ“MyDrive/Loras/james_genesis/dataset\n",
            "ğŸ“ˆ Se encontraron 37 imÃ¡genes con 10 repeticiones, equivalente a 370 pasos.\n",
            "ğŸ“‰ Divide 370 pasos en 2 batch size para obtener 185.0 pasos por epoch.\n",
            "ğŸ”® HabrÃ¡ 20 epochs, para un total de alrededor de 3700 pasos totales.\n",
            "\n",
            "âœ… Ya se ha realizado la instalaciÃ³n.\n",
            "\n",
            "ğŸ”„ El modelo ya ha sido descargado.\n",
            "\n",
            "\n",
            "ğŸ“„ ConfiguraciÃ³n guardada en /content/drive/MyDrive/Loras/james_genesis/training_config.toml\n",
            "ğŸ“„ ConfiguraciÃ³n de datos guardada en /content/drive/MyDrive/Loras/james_genesis/dataset_config.toml\n",
            "\n",
            "â­ Iniciando entrenador...\n",
            "\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1769631859.852943   12780 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1769631859.863293   12780 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1769631859.888580   12780 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769631859.888617   12780 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769631859.888621   12780 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769631859.888628   12780 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n",
            "Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n",
            "Loading settings from /content/drive/MyDrive/Loras/james_genesis/training_config.toml...\n",
            "/content/drive/MyDrive/Loras/james_genesis/training_config\n",
            "prepare tokenizer\n",
            "update token length: 225\n",
            "Loading dataset config from /content/drive/MyDrive/Loras/james_genesis/dataset_config.toml\n",
            "prepare images.\n",
            "found directory /content/drive/MyDrive/Loras/james_genesis/dataset contains 37 image files\n",
            "370 train images with repeating.\n",
            "0 reg images.\n",
            "no regularization images / æ­£å‰‡åŒ–ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ\n",
            "[Dataset 0]\n",
            "  batch_size: 2\n",
            "  resolution: (512, 512)\n",
            "  enable_bucket: True\n",
            "  network_multiplier: 1.0\n",
            "  min_bucket_reso: 256\n",
            "  max_bucket_reso: 1024\n",
            "  bucket_reso_steps: 64\n",
            "  bucket_no_upscale: False\n",
            "\n",
            "  [Subset 0 of Dataset 0]\n",
            "    image_dir: \"/content/drive/MyDrive/Loras/james_genesis/dataset\"\n",
            "    image_count: 37\n",
            "    num_repeats: 10\n",
            "    shuffle_caption: True\n",
            "    keep_tokens: 1\n",
            "    keep_tokens_separator: \n",
            "    caption_separator: ,\n",
            "    secondary_separator: None\n",
            "    enable_wildcard: False\n",
            "    caption_dropout_rate: 0.0\n",
            "    caption_dropout_every_n_epoches: 0\n",
            "    caption_tag_dropout_rate: 0.0\n",
            "    caption_prefix: None\n",
            "    caption_suffix: None\n",
            "    color_aug: False\n",
            "    flip_aug: False\n",
            "    face_crop_aug_range: None\n",
            "    random_crop: False\n",
            "    token_warmup_min: 1,\n",
            "    token_warmup_step: 0,\n",
            "    alpha_mask: False,\n",
            "    is_reg: False\n",
            "    class_tokens: None\n",
            "    caption_extension: .txt\n",
            "\n",
            "\n",
            "[Dataset 0]\n",
            "loading image sizes.\n",
            "100% 37/37 [00:00<00:00, 566.22it/s]\n",
            "make buckets\n",
            "number of images (including repeats) / å„bucketã®ç”»åƒæšæ•°ï¼ˆç¹°ã‚Šè¿”ã—å›æ•°ã‚’å«ã‚€ï¼‰\n",
            "bucket 0: resolution (512, 512), count: 370\n",
            "mean ar error (without repeats): 0.0\n",
            "preparing accelerator\n",
            "accelerator device: cuda\n",
            "loading model for process 0/1\n",
            "load StableDiffusion checkpoint: /content/sd-v1-5-pruned-noema-fp16.safetensors\n",
            "UNet2DConditionModel: 64, 8, 768, False, False\n",
            "loading u-net: <All keys matched successfully>\n",
            "loading vae: <All keys matched successfully>\n",
            "loading text encoder: <All keys matched successfully>\n",
            "Enable xformers for U-Net\n",
            "import network module: networks.lora\n",
            "[Dataset 0]\n",
            "caching latents.\n",
            "checking cache validity...\n",
            "100% 37/37 [00:00<00:00, 412737.36it/s]\n",
            "caching latents...\n",
            "100% 37/37 [00:17<00:00,  2.15it/s]\n",
            "create LoRA network. base dim (rank): 16, alpha: 8\n",
            "neuron dropout: p=None, rank dropout: p=None, module dropout: p=None\n",
            "create LoRA for Text Encoder:\n",
            "create LoRA for Text Encoder: 72 modules.\n",
            "create LoRA for U-Net: 192 modules.\n",
            "enable LoRA for text encoder: 72 modules\n",
            "enable LoRA for U-Net: 192 modules\n",
            "prepare optimizer, data loader etc.\n",
            "use 8-bit AdamW optimizer | {}\n",
            "override steps. steps for 20 epochs is / æŒ‡å®šã‚¨ãƒãƒƒã‚¯ã¾ã§ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°: 3700\n",
            "running training / å­¦ç¿’é–‹å§‹\n",
            "  num train images * repeats / å­¦ç¿’ç”»åƒã®æ•°Ã—ç¹°ã‚Šè¿”ã—å›æ•°: 370\n",
            "  num reg images / æ­£å‰‡åŒ–ç”»åƒã®æ•°: 0\n",
            "  num batches per epoch / 1epochã®ãƒãƒƒãƒæ•°: 185\n",
            "  num epochs / epochæ•°: 20\n",
            "  batch size per device / ãƒãƒƒãƒã‚µã‚¤ã‚º: 2\n",
            "  gradient accumulation steps / å‹¾é…ã‚’åˆè¨ˆã™ã‚‹ã‚¹ãƒ†ãƒƒãƒ—æ•° = 1\n",
            "  total optimization steps / å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—æ•°: 3700\n",
            "steps:   0% 0/3700 [00:00<?, ?it/s]\n",
            "epoch 1/20\n",
            "epoch is incremented. current_epoch: 0, epoch: 1\n",
            "epoch is incremented. current_epoch: 0, epoch: 1\n",
            "steps:   5% 185/3700 [02:19<44:13,  1.32it/s, avr_loss=0.0919]\n",
            "saving checkpoint: /content/drive/MyDrive/Loras/james_genesis/output/james_genesis-01.safetensors\n",
            "\n",
            "epoch 2/20\n",
            "epoch is incremented. current_epoch: 1, epoch: 2\n",
            "epoch is incremented. current_epoch: 1, epoch: 2\n",
            "steps:  10% 370/3700 [04:41<42:12,  1.32it/s, avr_loss=0.087] \n",
            "saving checkpoint: /content/drive/MyDrive/Loras/james_genesis/output/james_genesis-02.safetensors\n",
            "\n",
            "epoch 3/20\n",
            "epoch is incremented. current_epoch: 2, epoch: 3\n",
            "epoch is incremented. current_epoch: 2, epoch: 3\n",
            "steps:  15% 555/3700 [07:02<39:56,  1.31it/s, avr_loss=0.0839]\n",
            "saving checkpoint: /content/drive/MyDrive/Loras/james_genesis/output/james_genesis-03.safetensors\n",
            "\n",
            "epoch 4/20\n",
            "epoch is incremented. current_epoch: 3, epoch: 4\n",
            "epoch is incremented. current_epoch: 3, epoch: 4\n",
            "steps:  20% 740/3700 [09:24<37:38,  1.31it/s, avr_loss=0.0821]\n",
            "saving checkpoint: /content/drive/MyDrive/Loras/james_genesis/output/james_genesis-04.safetensors\n",
            "\n",
            "epoch 5/20\n",
            "epoch is incremented. current_epoch: 4, epoch: 5\n",
            "epoch is incremented. current_epoch: 4, epoch: 5\n",
            "steps:  25% 925/3700 [11:46<35:20,  1.31it/s, avr_loss=0.0802]\n",
            "saving checkpoint: /content/drive/MyDrive/Loras/james_genesis/output/james_genesis-05.safetensors\n",
            "\n",
            "epoch 6/20\n",
            "epoch is incremented. current_epoch: 5, epoch: 6\n",
            "epoch is incremented. current_epoch: 5, epoch: 6\n",
            "steps:  30% 1110/3700 [14:08<32:59,  1.31it/s, avr_loss=0.0708]\n",
            "saving checkpoint: /content/drive/MyDrive/Loras/james_genesis/output/james_genesis-06.safetensors\n",
            "\n",
            "epoch 7/20\n",
            "epoch is incremented. current_epoch: 6, epoch: 7\n",
            "epoch is incremented. current_epoch: 6, epoch: 7\n",
            "steps:  35% 1295/3700 [16:30<30:38,  1.31it/s, avr_loss=0.0768]\n",
            "saving checkpoint: /content/drive/MyDrive/Loras/james_genesis/output/james_genesis-07.safetensors\n",
            "\n",
            "epoch 8/20\n",
            "epoch is incremented. current_epoch: 7, epoch: 8\n",
            "epoch is incremented. current_epoch: 7, epoch: 8\n",
            "steps:  40% 1480/3700 [18:52<28:18,  1.31it/s, avr_loss=0.0693]\n",
            "saving checkpoint: /content/drive/MyDrive/Loras/james_genesis/output/james_genesis-08.safetensors\n",
            "\n",
            "epoch 9/20\n",
            "epoch is incremented. current_epoch: 8, epoch: 9\n",
            "epoch is incremented. current_epoch: 8, epoch: 9\n",
            "steps:  45% 1665/3700 [21:13<25:56,  1.31it/s, avr_loss=0.0782]\n",
            "saving checkpoint: /content/drive/MyDrive/Loras/james_genesis/output/james_genesis-09.safetensors\n",
            "\n",
            "epoch 10/20\n",
            "epoch is incremented. current_epoch: 9, epoch: 10\n",
            "epoch is incremented. current_epoch: 9, epoch: 10\n",
            "steps:  50% 1850/3700 [23:35<23:35,  1.31it/s, avr_loss=0.0716]\n",
            "saving checkpoint: /content/drive/MyDrive/Loras/james_genesis/output/james_genesis-10.safetensors\n",
            "\n",
            "epoch 11/20\n",
            "epoch is incremented. current_epoch: 10, epoch: 11\n",
            "epoch is incremented. current_epoch: 10, epoch: 11\n",
            "steps:  55% 2035/3700 [25:57<21:14,  1.31it/s, avr_loss=0.0722]\n",
            "saving checkpoint: /content/drive/MyDrive/Loras/james_genesis/output/james_genesis-11.safetensors\n",
            "removing old checkpoint: /content/drive/MyDrive/Loras/james_genesis/output/james_genesis-01.safetensors\n",
            "\n",
            "epoch 12/20\n",
            "epoch is incremented. current_epoch: 11, epoch: 12\n",
            "epoch is incremented. current_epoch: 11, epoch: 12\n",
            "steps:  60% 2220/3700 [28:19<18:52,  1.31it/s, avr_loss=0.075]\n",
            "saving checkpoint: /content/drive/MyDrive/Loras/james_genesis/output/james_genesis-12.safetensors\n",
            "removing old checkpoint: /content/drive/MyDrive/Loras/james_genesis/output/james_genesis-02.safetensors\n",
            "\n",
            "epoch 13/20\n",
            "epoch is incremented. current_epoch: 12, epoch: 13\n",
            "epoch is incremented. current_epoch: 12, epoch: 13\n",
            "steps:  65% 2405/3700 [30:40<16:31,  1.31it/s, avr_loss=0.0725]\n",
            "saving checkpoint: /content/drive/MyDrive/Loras/james_genesis/output/james_genesis-13.safetensors\n",
            "removing old checkpoint: /content/drive/MyDrive/Loras/james_genesis/output/james_genesis-03.safetensors\n",
            "\n",
            "epoch 14/20\n",
            "epoch is incremented. current_epoch: 13, epoch: 14\n",
            "epoch is incremented. current_epoch: 13, epoch: 14\n",
            "steps:  70% 2590/3700 [33:02<14:09,  1.31it/s, avr_loss=0.0688]\n",
            "saving checkpoint: /content/drive/MyDrive/Loras/james_genesis/output/james_genesis-14.safetensors\n",
            "removing old checkpoint: /content/drive/MyDrive/Loras/james_genesis/output/james_genesis-04.safetensors\n",
            "\n",
            "epoch 15/20\n",
            "epoch is incremented. current_epoch: 14, epoch: 15\n",
            "epoch is incremented. current_epoch: 14, epoch: 15\n",
            "steps:  75% 2775/3700 [35:24<11:48,  1.31it/s, avr_loss=0.071] \n",
            "saving checkpoint: /content/drive/MyDrive/Loras/james_genesis/output/james_genesis-15.safetensors\n",
            "removing old checkpoint: /content/drive/MyDrive/Loras/james_genesis/output/james_genesis-05.safetensors\n",
            "\n",
            "epoch 16/20\n",
            "epoch is incremented. current_epoch: 15, epoch: 16\n",
            "epoch is incremented. current_epoch: 15, epoch: 16\n",
            "steps:  80% 2960/3700 [37:45<09:26,  1.31it/s, avr_loss=0.0709]\n",
            "saving checkpoint: /content/drive/MyDrive/Loras/james_genesis/output/james_genesis-16.safetensors\n",
            "removing old checkpoint: /content/drive/MyDrive/Loras/james_genesis/output/james_genesis-06.safetensors\n",
            "\n",
            "epoch 17/20\n",
            "epoch is incremented. current_epoch: 16, epoch: 17\n",
            "epoch is incremented. current_epoch: 16, epoch: 17\n",
            "steps:  85% 3145/3700 [40:08<07:04,  1.31it/s, avr_loss=0.0674]\n",
            "saving checkpoint: /content/drive/MyDrive/Loras/james_genesis/output/james_genesis-17.safetensors\n",
            "removing old checkpoint: /content/drive/MyDrive/Loras/james_genesis/output/james_genesis-07.safetensors\n",
            "\n",
            "epoch 18/20\n",
            "epoch is incremented. current_epoch: 17, epoch: 18\n",
            "epoch is incremented. current_epoch: 17, epoch: 18\n",
            "steps:  90% 3330/3700 [42:29<04:43,  1.31it/s, avr_loss=0.0617]\n",
            "saving checkpoint: /content/drive/MyDrive/Loras/james_genesis/output/james_genesis-18.safetensors\n",
            "removing old checkpoint: /content/drive/MyDrive/Loras/james_genesis/output/james_genesis-08.safetensors\n",
            "\n",
            "epoch 19/20\n",
            "epoch is incremented. current_epoch: 18, epoch: 19\n",
            "epoch is incremented. current_epoch: 18, epoch: 19\n",
            "steps:  95% 3515/3700 [44:51<02:21,  1.31it/s, avr_loss=0.0624]\n",
            "saving checkpoint: /content/drive/MyDrive/Loras/james_genesis/output/james_genesis-19.safetensors\n",
            "removing old checkpoint: /content/drive/MyDrive/Loras/james_genesis/output/james_genesis-09.safetensors\n",
            "\n",
            "epoch 20/20\n",
            "epoch is incremented. current_epoch: 19, epoch: 20\n",
            "epoch is incremented. current_epoch: 19, epoch: 20\n",
            "steps: 100% 3700/3700 [47:12<00:00,  1.31it/s, avr_loss=0.0635]\n",
            "saving checkpoint: /content/drive/MyDrive/Loras/james_genesis/output/james_genesis-20.safetensors\n",
            "model saved.\n",
            "steps: 100% 3700/3700 [47:13<00:00,  1.31it/s, avr_loss=0.0635]\n",
            "\n",
            "\u001b[1mâœ… Done! Go download your Lora from Google Drive.\n",
            "There will be several files, you should try the latest version (the file with the largest number next to it)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import toml\n",
        "import shutil\n",
        "import zipfile\n",
        "from time import time\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "# These carry information from past executions\n",
        "if \"model_url\" in globals():\n",
        "  old_model_url = model_url\n",
        "else:\n",
        "  old_model_url = None\n",
        "if \"dependencies_installed\" not in globals():\n",
        "  dependencies_installed = False\n",
        "if \"model_file\" not in globals():\n",
        "  model_file = None\n",
        "\n",
        "# These may be set by other cells, some are legacy\n",
        "if \"custom_dataset\" not in globals():\n",
        "  custom_dataset = None\n",
        "if \"override_dataset_config_file\" not in globals():\n",
        "  override_dataset_config_file = None\n",
        "if \"override_config_file\" not in globals():\n",
        "  override_config_file = None\n",
        "if \"optimizer\" not in globals():\n",
        "  optimizer = \"AdamW8bit\"\n",
        "if \"optimizer_args\" not in globals():\n",
        "  optimizer_args = None\n",
        "if \"continue_from_lora\" not in globals():\n",
        "  continue_from_lora = \"\"\n",
        "if \"weighted_captions\" not in globals():\n",
        "  weighted_captions = False\n",
        "if \"adjust_tags\" not in globals():\n",
        "  adjust_tags = False\n",
        "if \"keep_tokens_weight\" not in globals():\n",
        "  keep_tokens_weight = 1.0\n",
        "\n",
        "COLAB = True # low ram\n",
        "XFORMERS = True\n",
        "SOURCE = \"https://github.com/uYouUs/sd-scripts\"\n",
        "COMMIT = None\n",
        "BETTER_EPOCH_NAMES = True\n",
        "LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "#@title ## ğŸš© Empieza AquÃ­\n",
        "\n",
        "#@markdown ### â–¶ï¸ Base\n",
        "#@markdown El nombre de tu proyecto tambiÃ©n es el nombre de la carpeta donde irÃ¡n tus imÃ¡genes. No se permiten espacios.\n",
        "nombre_proyecto = \"james_genesis\" #@param {type:\"string\"}\n",
        "project_name = nombre_proyecto.strip()\n",
        "#@markdown La estructura de carpetas no importa y es por comodidad. AsegÃºrate de siempre elegir la misma. Me gusta organizar por proyecto.\n",
        "estructura_de_carpetas = \"Organizar por proyecto (MyDrive/Loras/nombre_proyecto/dataset)\" #@param [\"Organizar por categorÃ­a (MyDrive/lora_training/datasets/nombre_proyecto)\", \"Organizar por proyecto (MyDrive/Loras/nombre_proyecto/dataset)\"]\n",
        "folder_structure = estructura_de_carpetas\n",
        "#@markdown Decidir el modelo base de entrenamiento. Los modelos por defecto producen los resultados mÃ¡s limpios y consistentes. Puedes cambiarlo por un modelo propio si lo deseas.\n",
        "modelo_de_entrenamiento = \"Stable Diffusion (sd-v1-5-pruned-noema-fp16.safetensors)\" #@param [\"Anime (animefull-final-pruned-fp16.safetensors)\", \"AnyLora (AnyLoRA_noVae_fp16-pruned.ckpt)\", \"Stable Diffusion (sd-v1-5-pruned-noema-fp16.safetensors)\"]\n",
        "opcional_enlace_a_modelo_propio = \"\" #@param {type:\"string\"}\n",
        "modelo_propio_basado_en_sd2 = False #@param {type:\"boolean\"}\n",
        "custom_model_is_based_on_sd2 = modelo_propio_basado_en_sd2\n",
        "\n",
        "if opcional_enlace_a_modelo_propio:\n",
        "  model_url = opcional_enlace_a_modelo_propio\n",
        "elif \"AnyLora\" in modelo_de_entrenamiento:\n",
        "  model_url = \"https://huggingface.co/Lykon/AnyLoRA/resolve/main/AnyLoRA_noVae_fp16-pruned.ckpt\"\n",
        "elif \"Anime\" in modelo_de_entrenamiento:\n",
        "  model_url = \"https://huggingface.co/hollowstrawberry/stable-diffusion-guide/resolve/main/models/animefull-final-pruned-fp16.safetensors\"\n",
        "else:\n",
        "  model_url = \"https://huggingface.co/hollowstrawberry/stable-diffusion-guide/resolve/main/models/sd-v1-5-pruned-noema-fp16.safetensors\"\n",
        "\n",
        "#@markdown ### â–¶ï¸ Procesamiento <p>\n",
        "#@markdown La resoluciÃ³n de 512 es estÃ¡ndar en Stable Diffusion 1.5. No es necesario recortar o achicar, el proceso es automÃ¡tico.\n",
        "resolucion = 512 #@param {type:\"slider\", min:512, max:1024, step:128}\n",
        "resolution = resolucion\n",
        "#@markdown Esta opciÃ³n va a voltear tus imÃ¡genes para asÃ­ tener el doble y aprender mejor. <p>\n",
        "#@markdown **Desactiva esto si te importan los elementos asimÃ©tricos en tu Lora.**\n",
        "flip_aug = False #@param {type:\"boolean\"}\n",
        "#markdown Leave empty for no captions.\n",
        "caption_extension = \".txt\" #param {type:\"string\"}\n",
        "#@markdown Mezclar las tags de anime ayuda al aprendizaje. Una tag de activaciÃ³n va al inicio de cada archivo de texto y no se mezclarÃ¡.\n",
        "mezclar_tags = True #@param {type:\"boolean\"}\n",
        "shuffle_caption = mezclar_tags\n",
        "tags_de_activacion = \"1\" #@param [0,1,2,3]\n",
        "keep_tokens = int(tags_de_activacion)\n",
        "\n",
        "#@markdown ### â–¶ï¸ Pasos <p>\n",
        "#@markdown Tus imÃ¡genes se repetirÃ¡n este nÃºmero de veces durante el entrenamiento. Recomiendo que el valor total sea entre 200 y 400.\n",
        "num_repeats = 10 #@param {type:\"number\"}\n",
        "#@markdown CuÃ¡nto tiempo deseas entrenar. Un buen punto de partida puede ser alrededor de 10 epochs o alrededor de 2000 pasos. <p>\n",
        "#@markdown Un epoch es una cantidad de pasos igual a: tu cantidad de imÃ¡genes multipliccada por sus repeticiones, y dividido en el batch size.\n",
        "unidad_preferida = \"Epochs\" #@param [\"Epochs\", \"Pasos\"]\n",
        "cuantos = 20 #@param {type:\"number\"}\n",
        "max_train_epochs = cuantos if unidad_preferida == \"Epochs\" else None\n",
        "max_train_steps = cuantos if unidad_preferida == \"Pasos\" else None\n",
        "#@markdown Guardar mÃ¡s epochs te permitirÃ¡ comparar mejor el progreso de tu Lora.\n",
        "guardar_cada_cuantos_epochs = 1 #@param {type:\"number\"}\n",
        "save_every_n_epochs = guardar_cada_cuantos_epochs\n",
        "guardar_solo_ultimos_epochs = 10 #@param {type:\"number\"}\n",
        "keep_only_last_n_epochs = guardar_solo_ultimos_epochs\n",
        "if not save_every_n_epochs:\n",
        "  save_every_n_epochs = max_train_epochs\n",
        "if not keep_only_last_n_epochs:\n",
        "  keep_only_last_n_epochs = max_train_epochs\n",
        "#@markdown Un batch size mayor hace el entrenamiento mÃ¡s rÃ¡pido, pero puede empeorar el aprendizaje. Se recomienda 2 o 3.\n",
        "batch_size = 2 #@param {type:\"slider\", min:1, max:8, step:1}\n",
        "train_batch_size = batch_size\n",
        "\n",
        "#@markdown ### â–¶ï¸ Aprendizaje\n",
        "#@markdown La tasa de aprendizaje es lo mÃ¡s importante. Si deseas entrenar mÃ¡s lento con muchas imÃ¡genes, o si tienes un dim y alpha altos, usa un unet de 2e-4 o menor. <p>\n",
        "#@markdown El text encoder ayuda a tu Lora a aprender conceptos un poco mejor. Se recomienda la mitad o un quinto del unet. Puedes dejarlo en 0 para algunos estilos.\n",
        "aprendizaje_unet = 2e-4 #@param {type:\"number\"}\n",
        "unet_lr = aprendizaje_unet\n",
        "aprendizaje_text_encoder = 1e-4 #@param {type:\"number\"}\n",
        "text_encoder_lr = aprendizaje_text_encoder\n",
        "#@markdown El scheduler es el algoritmo matemÃ¡tico que guiarÃ¡ el entrenamiento. Para personajes recomiendo `cosine_with_restarts` con un valor de 3. Si no estÃ¡s seguro ponlo en `constant` e ignora el valor.\n",
        "scheduler = \"cosine_with_restarts\" #@param [\"constant\", \"cosine\", \"cosine_with_restarts\", \"constant_with_warmup\", \"linear\", \"polynomial\"]\n",
        "lr_scheduler = scheduler\n",
        "valor_de_scheduler = 3 #@param {type:\"number\"}\n",
        "lr_scheduler_number = valor_de_scheduler\n",
        "lr_scheduler_num_cycles = lr_scheduler_number if lr_scheduler == \"cosine_with_restarts\" else 0\n",
        "lr_scheduler_power = lr_scheduler_number if lr_scheduler == \"polynomial\" else 0\n",
        "#@markdown Pasos de calentamiento durante el entrenamiento para un inicio eficiente. Recomiendo dejarlo en 5%.\n",
        "calentamiento = 0.05 #@param {type:\"slider\", min:0.0, max:0.5, step:0.01}\n",
        "lr_warmup_ratio = calentamiento\n",
        "lr_warmup_steps = 0\n",
        "#@markdown Nueva funciÃ³n que hace el aprendizaje mucho mÃ¡s eficiente. Puede que tus Loras estÃ©n listos en la mitad de epochs. Se usarÃ¡ un valor de 5.0 como en la [investigaciÃ³n](https://arxiv.org/abs/2303.09556).\n",
        "min_snr_gamma = True #@param {type:\"boolean\"}\n",
        "min_snr_gamma_value = 5.0 if min_snr_gamma else None\n",
        "\n",
        "#@markdown ### â–¶ï¸ Estructura\n",
        "#@markdown LoRA es el clÃ¡sico y Ãºtil para muchos usos. LoCon es bueno con estilos ya que aprende con mÃ¡s capas.\n",
        "lora_type = \"LoRA\" #@param [\"LoRA\", \"LoCon\"]\n",
        "\n",
        "#@markdown AquÃ­ hay algunos valores recomendados para las opciones de abajo:\n",
        "\n",
        "#@markdown | type | network_dim | network_alpha | conv_dim | conv_alpha |\n",
        "#@markdown | :---: | :---: | :---: | :---: | :---: |\n",
        "#@markdown | LoRA | 16 | 8 |   |   |\n",
        "#@markdown | LoCon | 16 | 8 | 8 | 4 |\n",
        "\n",
        "#@markdown Un dim mayor equivale a un Lora mÃ¡s grande, pero no siempre es mejor. Se recomienda de 8 a 32, con un alpha igual a la mitad del dim.\n",
        "network_dim = 16 #@param {type:\"slider\", min:1, max:128, step:1}\n",
        "network_alpha = 8 #@param {type:\"slider\", min:1, max:128, step:1}\n",
        "#@markdown Los siguientes sÃ³lo aplican a las capas adicionales de LoCon.\n",
        "conv_dim = 8 #@param {type:\"slider\", min:1, max:64, step:1}\n",
        "conv_alpha = 4 #@param {type:\"slider\", min:1, max:64, step:1}\n",
        "\n",
        "network_module = \"networks.lora\"\n",
        "network_args = None\n",
        "if lora_type.lower() == \"locon\":\n",
        "  network_args = [f\"conv_dim={conv_dim}\", f\"conv_alpha={conv_alpha}\"]\n",
        "\n",
        "#@markdown ### â–¶ï¸ Listo\n",
        "#@markdown Ahora puedes correr esta celda apretando el botÃ³n circular a la izquierda. Â¡Buena suerte!\n",
        "\n",
        "\n",
        "# ğŸ‘©â€ğŸ’» Cool code goes here\n",
        "\n",
        "if optimizer.lower() == \"prodigy\" or \"dadapt\" in optimizer.lower():\n",
        "  if override_values_for_dadapt_and_prodigy:\n",
        "    unet_lr = 0.5\n",
        "    text_encoder_lr = 0.5\n",
        "    lr_scheduler = \"constant_with_warmup\"\n",
        "    lr_warmup_ratio = 0.05\n",
        "    network_alpha = network_dim\n",
        "\n",
        "  if not optimizer_args:\n",
        "    optimizer_args = [\"decouple=True\",\"weight_decay=0.01\",\"betas=[0.9,0.999]\"]\n",
        "    if optimizer == \"Prodigy\":\n",
        "      optimizer_args.extend([\"d_coef=2\",\"use_bias_correction=True\"])\n",
        "      if lr_warmup_ratio > 0:\n",
        "        optimizer_args.append(\"safeguard_warmup=True\")\n",
        "      else:\n",
        "        optimizer_args.append(\"safeguard_warmup=False\")\n",
        "\n",
        "root_dir = \"/content\" if COLAB else \"~/Loras\"\n",
        "deps_dir = os.path.join(root_dir, \"deps\")\n",
        "repo_dir = os.path.join(root_dir, \"kohya-trainer\")\n",
        "\n",
        "if \"/Loras\" in folder_structure:\n",
        "  main_dir      = os.path.join(root_dir, \"drive/MyDrive/Loras\") if COLAB else root_dir\n",
        "  log_folder    = os.path.join(main_dir, \"_logs\")\n",
        "  config_folder = os.path.join(main_dir, project_name)\n",
        "  images_folder = os.path.join(main_dir, project_name, \"dataset\")\n",
        "  output_folder = os.path.join(main_dir, project_name, \"output\")\n",
        "else:\n",
        "  main_dir      = os.path.join(root_dir, \"drive/MyDrive/lora_training\") if COLAB else root_dir\n",
        "  images_folder = os.path.join(main_dir, \"datasets\", project_name)\n",
        "  output_folder = os.path.join(main_dir, \"output\", project_name)\n",
        "  config_folder = os.path.join(main_dir, \"config\", project_name)\n",
        "  log_folder    = os.path.join(main_dir, \"log\")\n",
        "\n",
        "config_file = os.path.join(config_folder, \"training_config.toml\")\n",
        "dataset_config_file = os.path.join(config_folder, \"dataset_config.toml\")\n",
        "accelerate_config_file = os.path.join(repo_dir, \"accelerate_config/config.yaml\")\n",
        "\n",
        "def install_dependencies():\n",
        "  os.chdir(root_dir)\n",
        "  !git clone {SOURCE} {repo_dir}\n",
        "  os.chdir(repo_dir)\n",
        "  if COMMIT:\n",
        "    !git reset --hard {COMMIT}\n",
        "  !wget https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/train_network_wrapper.py -q -O train_network_wrapper.py\n",
        "  !wget https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/dracula.py -q -O dracula.py\n",
        "\n",
        "  !apt -y update -qq\n",
        "  !apt -y install aria2 -qq\n",
        "  !pip install accelerate==0.25.0 transformers==4.36.2 diffusers[torch]==0.25.0 ftfy==6.1.1 \\\n",
        "    opencv-python==4.8.1.78 einops==0.7.0 pytorch-lightning==1.9.0 bitsandbytes==0.43.0 \\\n",
        "    prodigyopt==1.0 lion-pytorch==0.0.6 tensorboard safetensors==0.4.2 altair==4.2.2 \\\n",
        "    easygui==0.98.3 toml==0.10.2 voluptuous==0.13.1 huggingface-hub==0.20.1 imagesize==1.4.1 \\\n",
        "    rich==13.7.1 torch==2.4.1+cu121 triton\n",
        "  !pip install -e .\n",
        "  if XFORMERS:\n",
        "    !pip install xformers==0.0.28.post1\n",
        "\n",
        "  # patch kohya for minor stuff\n",
        "  if COLAB:\n",
        "    !sed -i \"s@cpu@cuda@\" library/model_util.py # low ram\n",
        "  if LOAD_TRUNCATED_IMAGES:\n",
        "    !sed -i 's/from PIL import Image/from PIL import Image, ImageFile\\nImageFile.LOAD_TRUNCATED_IMAGES=True/g' library/train_util.py # fix truncated jpegs error\n",
        "  if BETTER_EPOCH_NAMES:\n",
        "    !sed -i 's/{:06d}/{:02d}/g' library/train_util.py # make epoch names shorter\n",
        "    !sed -i 's/\".\" + args.save_model_as)/\"-{:02d}.\".format(num_train_epochs) + args.save_model_as)/g' train_network.py # name of the last epoch will match the rest\n",
        "\n",
        "  from accelerate.utils import write_basic_config\n",
        "  if not os.path.exists(accelerate_config_file):\n",
        "    write_basic_config(save_location=accelerate_config_file)\n",
        "\n",
        "  os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "  os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"\n",
        "  os.environ[\"SAFETENSORS_FAST_GPU\"] = \"1\"\n",
        "\n",
        "def validate_dataset():\n",
        "  global lr_warmup_steps, lr_warmup_ratio, caption_extension, keep_tokens, keep_tokens_weight, weighted_captions, adjust_tags\n",
        "  supported_types = (\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\")\n",
        "\n",
        "  print(\"\\nğŸ’¿ Revisando archivos...\")\n",
        "  if not project_name.strip() or any(c in project_name for c in \" .()\\\"'\\\\/\"):\n",
        "    print(\"ğŸ’¥ Error: Por favor elije un nombre de proyecto vÃ¡lido.\")\n",
        "    return\n",
        "\n",
        "  if custom_dataset:\n",
        "    try:\n",
        "      datconf = toml.loads(custom_dataset)\n",
        "      datasets = [d for d in datconf[\"datasets\"][0][\"subsets\"]]\n",
        "    except:\n",
        "      print(f\"ğŸ’¥ Error: Â¡Tu configuraciÃ³n de datos propia es invÃ¡lida o tiene errores! Por favor revisa el ejemplo original.\")\n",
        "      return\n",
        "    reg = [d.get(\"image_dir\") for d in datasets if d.get(\"is_reg\", False)]\n",
        "    datasets_dict = {d[\"image_dir\"]: d[\"num_repeats\"] for d in datasets}\n",
        "    folders = datasets_dict.keys()\n",
        "    files = [f for folder in folders for f in os.listdir(folder)]\n",
        "    images_repeats = {folder: (len([f for f in os.listdir(folder) if f.lower().endswith(supported_types)]), datasets_dict[folder]) for folder in folders}\n",
        "  else:\n",
        "    reg = []\n",
        "    folders = [images_folder]\n",
        "    files = os.listdir(images_folder)\n",
        "    images_repeats = {images_folder: (len([f for f in files if f.lower().endswith(supported_types)]), num_repeats)}\n",
        "\n",
        "  for folder in folders:\n",
        "    if not os.path.exists(folder):\n",
        "      print(f\"ğŸ’¥ Error: La carpeta {folder.replace('/content/drive/', '')} no existe.\")\n",
        "      return\n",
        "  for folder, (img, rep) in images_repeats.items():\n",
        "    if not img:\n",
        "      print(f\"ğŸ’¥ Error: La carpeta {folder.replace('/content/drive/', '')} estÃ¡ vacÃ­a.\")\n",
        "      return\n",
        "  for f in files:\n",
        "    if not f.lower().endswith((\".txt\", \".npz\")) and not f.lower().endswith(supported_types):\n",
        "      print(f\"ğŸ’¥ Error: Archivo invÃ¡lido encontrado: \\\"{f}\\\". Abortando.\")\n",
        "      return\n",
        "\n",
        "  if not [txt for txt in files if txt.lower().endswith(\".txt\")]:\n",
        "    caption_extension = \"\"\n",
        "  if continue_from_lora and not (continue_from_lora.endswith(\".safetensors\") and os.path.exists(continue_from_lora)):\n",
        "    print(f\"ğŸ’¥ Error: Archivo de continuar_lora invÃ¡lido. Ejemplo: /content/drive/MyDrive/Loras/ejemplo.safetensors\")\n",
        "    return\n",
        "\n",
        "  pre_steps_per_epoch = sum(img*rep for (img, rep) in images_repeats.values())\n",
        "  steps_per_epoch = pre_steps_per_epoch/train_batch_size\n",
        "  total_steps = max_train_steps or int(max_train_epochs*steps_per_epoch)\n",
        "  estimated_epochs = int(total_steps/steps_per_epoch)\n",
        "  lr_warmup_steps = int(total_steps*lr_warmup_ratio)\n",
        "\n",
        "  for folder, (img, rep) in images_repeats.items():\n",
        "    print(\"ğŸ“\"+folder.replace(\"/content/drive/\", \"\") + (\" (RegularizaciÃ³n)\" if folder in reg else \"\"))\n",
        "  print(f\"ğŸ“ˆ Se encontraron {img} imÃ¡genes con {rep} repeticiones, equivalente a {img*rep} pasos.\")\n",
        "  print(f\"ğŸ“‰ Divide {pre_steps_per_epoch} pasos en {train_batch_size} batch size para obtener {steps_per_epoch} pasos por epoch.\")\n",
        "  if max_train_epochs:\n",
        "    print(f\"ğŸ”® HabrÃ¡ {max_train_epochs} epochs, para un total de alrededor de {total_steps} pasos totales.\")\n",
        "  else:\n",
        "    print(f\"ğŸ”® HabrÃ¡ {total_steps} pasos, divididos en {estimated_epochs} epochs y un poco mÃ¡s.\")\n",
        "\n",
        "  if total_steps > 10000:\n",
        "    print(\"ğŸ’¥ Error: Tus pasos totales on muy altos. Probablemente cometiste un error. Abortando...\")\n",
        "    return\n",
        "\n",
        "  if adjust_tags:\n",
        "    print(f\"\\nğŸ“ Weighted tags: {'ON' if weighted_captions else 'OFF'}\")\n",
        "    if weighted_captions:\n",
        "      print(f\"ğŸ“ Will use {keep_tokens_weight} weight on {keep_tokens} activation tag(s)\")\n",
        "    print(\"ğŸ“ Adjusting tags...\")\n",
        "    adjust_weighted_tags(folders, keep_tokens, keep_tokens_weight, weighted_captions)\n",
        "\n",
        "  return True\n",
        "\n",
        "def adjust_weighted_tags(folders, keep_tokens: int, keep_tokens_weight: float, weighted_captions: bool):\n",
        "  weighted_tag = re.compile(r\"\\((.+?):[.\\d]+\\)(,|$)\")\n",
        "  for folder in folders:\n",
        "    for txt in [f for f in os.listdir(folder) if f.lower().endswith(\".txt\")]:\n",
        "      with open(os.path.join(folder, txt), 'r') as f:\n",
        "        content = f.read()\n",
        "      # reset previous changes\n",
        "      content = content.replace('\\\\', '')\n",
        "      content = weighted_tag.sub(r'\\1\\2', content)\n",
        "      if weighted_captions:\n",
        "        # re-apply changes\n",
        "        content = content.replace(r'(', r'\\(').replace(r')', r'\\)').replace(r':', r'\\:')\n",
        "        if keep_tokens_weight > 1:\n",
        "          tags = [s.strip() for s in content.split(\",\")]\n",
        "          for i in range(min(keep_tokens, len(tags))):\n",
        "            tags[i] = f'({tags[i]}:{keep_tokens_weight})'\n",
        "          content = \", \".join(tags)\n",
        "      with open(os.path.join(folder, txt), 'w') as f:\n",
        "        f.write(content)\n",
        "\n",
        "def create_config():\n",
        "  global dataset_config_file, config_file, model_file\n",
        "\n",
        "  if override_config_file:\n",
        "    config_file = override_config_file\n",
        "    print(f\"\\nâ­• Usando configuraciÃ³n propia {config_file}\")\n",
        "  else:\n",
        "    config_dict = {\n",
        "      \"additional_network_arguments\": {\n",
        "        \"unet_lr\": unet_lr,\n",
        "        \"text_encoder_lr\": text_encoder_lr,\n",
        "        \"network_dim\": network_dim,\n",
        "        \"network_alpha\": network_alpha,\n",
        "        \"network_module\": network_module,\n",
        "        \"network_args\": network_args,\n",
        "        \"network_train_unet_only\": True if text_encoder_lr == 0 else None,\n",
        "        \"network_weights\": continue_from_lora if continue_from_lora else None\n",
        "      },\n",
        "      \"optimizer_arguments\": {\n",
        "        \"learning_rate\": unet_lr,\n",
        "        \"lr_scheduler\": lr_scheduler,\n",
        "        \"lr_scheduler_num_cycles\": lr_scheduler_num_cycles if lr_scheduler == \"cosine_with_restarts\" else None,\n",
        "        \"lr_scheduler_power\": lr_scheduler_power if lr_scheduler == \"polynomial\" else None,\n",
        "        \"lr_warmup_steps\": lr_warmup_steps if lr_scheduler != \"constant\" else None,\n",
        "        \"optimizer_type\": optimizer,\n",
        "        \"optimizer_args\": optimizer_args if optimizer_args else None,\n",
        "      },\n",
        "      \"training_arguments\": {\n",
        "        \"max_train_steps\": max_train_steps,\n",
        "        \"max_train_epochs\": max_train_epochs,\n",
        "        \"save_every_n_epochs\": save_every_n_epochs,\n",
        "        \"save_last_n_epochs\": keep_only_last_n_epochs,\n",
        "        \"train_batch_size\": train_batch_size,\n",
        "        \"noise_offset\": None,\n",
        "        \"clip_skip\": 2,\n",
        "        \"min_snr_gamma\": min_snr_gamma_value,\n",
        "        \"weighted_captions\": weighted_captions,\n",
        "        \"seed\": 42,\n",
        "        \"max_token_length\": 225,\n",
        "        \"xformers\": XFORMERS,\n",
        "        \"lowram\": COLAB,\n",
        "        \"max_data_loader_n_workers\": 8,\n",
        "        \"persistent_data_loader_workers\": True,\n",
        "        \"save_precision\": \"fp16\",\n",
        "        \"mixed_precision\": \"fp16\",\n",
        "        \"output_dir\": output_folder,\n",
        "        \"logging_dir\": log_folder,\n",
        "        \"output_name\": project_name,\n",
        "        \"log_prefix\": project_name,\n",
        "      },\n",
        "      \"model_arguments\": {\n",
        "        \"pretrained_model_name_or_path\": model_file,\n",
        "        \"v2\": custom_model_is_based_on_sd2,\n",
        "        \"v_parameterization\": True if custom_model_is_based_on_sd2 else None,\n",
        "      },\n",
        "      \"saving_arguments\": {\n",
        "        \"save_model_as\": \"safetensors\",\n",
        "      },\n",
        "      \"dreambooth_arguments\": {\n",
        "        \"prior_loss_weight\": 1.0,\n",
        "      },\n",
        "      \"dataset_arguments\": {\n",
        "        \"cache_latents\": True,\n",
        "      },\n",
        "    }\n",
        "\n",
        "    for key in config_dict:\n",
        "      if isinstance(config_dict[key], dict):\n",
        "        config_dict[key] = {k: v for k, v in config_dict[key].items() if v is not None}\n",
        "\n",
        "    with open(config_file, \"w\") as f:\n",
        "      f.write(toml.dumps(config_dict))\n",
        "    print(f\"\\nğŸ“„ ConfiguraciÃ³n guardada en {config_file}\")\n",
        "\n",
        "  if override_dataset_config_file:\n",
        "    dataset_config_file = override_dataset_config_file\n",
        "    print(f\"â­• Usando configuraciÃ³n de datos propia {dataset_config_file}\")\n",
        "  else:\n",
        "    dataset_config_dict = {\n",
        "      \"general\": {\n",
        "        \"resolution\": resolution,\n",
        "        \"shuffle_caption\": shuffle_caption,\n",
        "        \"keep_tokens\": keep_tokens,\n",
        "        \"flip_aug\": flip_aug,\n",
        "        \"caption_extension\": caption_extension,\n",
        "        \"enable_bucket\": True,\n",
        "        \"bucket_reso_steps\": 64,\n",
        "        \"bucket_no_upscale\": False,\n",
        "        \"min_bucket_reso\": 320 if resolution > 640 else 256,\n",
        "        \"max_bucket_reso\": 1280 if resolution > 640 else 1024,\n",
        "      },\n",
        "      \"datasets\": toml.loads(custom_dataset)[\"datasets\"] if custom_dataset else [\n",
        "        {\n",
        "          \"subsets\": [\n",
        "            {\n",
        "              \"num_repeats\": num_repeats,\n",
        "              \"image_dir\": images_folder,\n",
        "              \"class_tokens\": None if caption_extension else project_name\n",
        "            }\n",
        "          ]\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "\n",
        "    for key in dataset_config_dict:\n",
        "      if isinstance(dataset_config_dict[key], dict):\n",
        "        dataset_config_dict[key] = {k: v for k, v in dataset_config_dict[key].items() if v is not None}\n",
        "\n",
        "    with open(dataset_config_file, \"w\") as f:\n",
        "      f.write(toml.dumps(dataset_config_dict))\n",
        "    print(f\"ğŸ“„ ConfiguraciÃ³n de datos guardada en {dataset_config_file}\")\n",
        "\n",
        "def download_model():\n",
        "  global old_model_url, model_url, model_file\n",
        "  real_model_url = model_url.strip()\n",
        "\n",
        "  if real_model_url.lower().endswith((\".ckpt\", \".safetensors\")):\n",
        "    model_file = f\"/content{real_model_url[real_model_url.rfind('/'):]}\"\n",
        "  else:\n",
        "    model_file = \"/content/downloaded_model.safetensors\"\n",
        "    if os.path.exists(model_file):\n",
        "      !rm \"{model_file}\"\n",
        "\n",
        "  if m := re.search(r\"(?:https?://)?(?:www\\.)?huggingface\\.co/[^/]+/[^/]+/blob\", model_url):\n",
        "    real_model_url = real_model_url.replace(\"blob\", \"resolve\")\n",
        "  elif m := re.search(r\"(?:https?://)?(?:www\\.)?civitai\\.com/models/([0-9]+)\", model_url):\n",
        "    real_model_url = f\"https://civitai.com/api/download/models/{m.group(1)}\"\n",
        "\n",
        "  !aria2c \"{real_model_url}\" --console-log-level=warn -c -s 16 -x 16 -k 10M -d / -o \"{model_file}\"\n",
        "\n",
        "  if model_file.lower().endswith(\".safetensors\"):\n",
        "    from safetensors.torch import load_file as load_safetensors\n",
        "    try:\n",
        "      test = load_safetensors(model_file)\n",
        "      del test\n",
        "    except:\n",
        "      #if \"HeaderTooLarge\" in str(e):\n",
        "      new_model_file = os.path.splitext(model_file)[0]+\".ckpt\"\n",
        "      !mv \"{model_file}\" \"{new_model_file}\"\n",
        "      model_file = new_model_file\n",
        "      print(f\"Modelo renombrado a {os.path.splitext(model_file)[0]}.ckpt\")\n",
        "\n",
        "  if model_file.lower().endswith(\".ckpt\"):\n",
        "    from torch import load as load_ckpt\n",
        "    try:\n",
        "      test = load_ckpt(model_file)\n",
        "      del test\n",
        "    except:\n",
        "      return False\n",
        "\n",
        "  return True\n",
        "\n",
        "def main():\n",
        "  global dependencies_installed\n",
        "\n",
        "  if COLAB and not os.path.exists('/content/drive'):\n",
        "    from google.colab import drive\n",
        "    print(\"ğŸ“‚ Conectando a Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "  for dir in (main_dir, deps_dir, repo_dir, log_folder, images_folder, output_folder, config_folder):\n",
        "    os.makedirs(dir, exist_ok=True)\n",
        "\n",
        "  if not validate_dataset():\n",
        "    return\n",
        "\n",
        "  if not dependencies_installed:\n",
        "    print(\"\\nğŸ­ Instalando...\\n\")\n",
        "    t0 = time()\n",
        "    install_dependencies()\n",
        "    t1 = time()\n",
        "    dependencies_installed = True\n",
        "    print(f\"\\nâœ… InstalaciÃ³n completada en {int(t1-t0)} segundos.\")\n",
        "  else:\n",
        "    print(\"\\nâœ… Ya se ha realizado la instalaciÃ³n.\")\n",
        "\n",
        "  if old_model_url != model_url or not model_file or not os.path.exists(model_file):\n",
        "    print(\"\\nğŸ”„ Descargando modelo...\")\n",
        "    if not download_model():\n",
        "      print(\"\\nğŸ’¥ Error: El modelo que elegiste es invÃ¡lido o estÃ¡ corrupto, o no se pudo encontrar. Recomiendo usar un enlace de huggingface o civitai.\")\n",
        "      return\n",
        "    print()\n",
        "  else:\n",
        "    print(\"\\nğŸ”„ El modelo ya ha sido descargado.\\n\")\n",
        "\n",
        "  create_config()\n",
        "\n",
        "  print(\"\\nâ­ Iniciando entrenador...\\n\")\n",
        "  os.chdir(repo_dir)\n",
        "\n",
        "  !accelerate launch --config_file={accelerate_config_file} --num_cpu_threads_per_process=1 train_network_wrapper.py --dataset_config={dataset_config_file} --config_file={config_file}\n",
        "\n",
        "main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBMUJ7BuvNcn"
      },
      "source": [
        "## *ï¸âƒ£ Extras\n",
        "\n",
        "You can run these before starting the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "sy9jU2yrdYar"
      },
      "outputs": [],
      "source": [
        "#@markdown ### ğŸ”® Optimizador\n",
        "#@markdown Si corres esta celda cambiarÃ¡s el optimizador usado en el entrenamiento. Sino, el por defecto es `AdamW8bit`, el cual es recomendado.<p>\n",
        "#@markdown * Dadapt y Prodigy manejan la tasa de aprendizaje de forma automÃ¡tica, y son muy buenos con datasets pequeÃ±os. Puedes usarlos sin cambiar nada mÃ¡s aquÃ­.<p>\n",
        "#@markdown Con Dadapt o Prodigy, los siguientes valores serÃ¡n sobreescritos:<p>\n",
        "#@markdown `learning_rate=0.5`, `network_alpha=network_dim`, `lr_scheduler=\"constant_with_warmup\"`, `lr_warmup_ratio=0.05`<p>\n",
        "#@markdown Con Dadapt o Prodigy, si `optimizer_args` estÃ¡ vacÃ­o su valor serÃ¡ `decouple=True, weight_decay=0.01, betas=[0.9,0.999]`<p>\n",
        "#@markdown Y ademÃ¡s con Prodigy: `d_coef=2, use_bias_correction=True, safeguard_warmup=True`<p>\n",
        "optimizer = \"Prodigy\" #@param [\"AdamW8bit\", \"Prodigy\", \"DAdaptation\", \"DadaptAdam\", \"DadaptLion\", \"AdamW\", \"Lion\", \"SGDNesterov\", \"SGDNesterov8bit\", \"AdaFactor\"]\n",
        "optimizer_args = \"\" #@param {type:\"string\"}\n",
        "splitter = \", \" if \", \" in optimizer_args else \",\"\n",
        "optimizer_args = [a.strip() for a in optimizer_args.split(splitter) if a]\n",
        "override_values_for_dadapt_and_prodigy = True #@param {type:\"boolean\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wd4916Eu1tb9"
      },
      "source": [
        "### ğŸ“š MÃºltiples carpetas\n",
        "**Para usuarios avanzados:** Antes de iniciar el entrenamiento, puedes editar y correr la celda aquÃ­ abajo, la cual tiene un ejemplo para definir tus propias carpetas de imÃ¡genes con diferentes repeticiones.\n",
        "\n",
        "(El nÃºmero de repeticiones de la celda principal serÃ¡ ignorado, y tambiÃ©n la carpeta principal con el nombre del proyecto)\n",
        "\n",
        "Puedes hacer que una carpeta contenga imÃ¡genes de regularizaciÃ³n con la frase `is_reg = true`\n",
        "TambiÃ©n puedes poner distintos `keep_tokens`, `flip_aug`, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y037lagnJWmn"
      },
      "outputs": [],
      "source": [
        "custom_dataset = \"\"\"\n",
        "[[datasets]]\n",
        "\n",
        "[[datasets.subsets]]\n",
        "image_dir = \"/content/drive/MyDrive/Loras/ejemplo/dataset/imagenes_buenas\"\n",
        "num_repeats = 3\n",
        "\n",
        "[[datasets.subsets]]\n",
        "image_dir = \"/content/drive/MyDrive/Loras/ejemplo/dataset/imagenes_normales\"\n",
        "num_repeats = 1\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W84Jxf-U2TIU"
      },
      "outputs": [],
      "source": [
        "custom_dataset = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-Yq5mNvcCy2l"
      },
      "outputs": [],
      "source": [
        "#@markdown ### ğŸ¤“ Otros\n",
        "#@markdown Estas opciones son innecesarias, pero pueden servir a algunas personas.\n",
        "\n",
        "#@markdown Weighted captions is a new feature that allows you to use (parentheses) to give more weight to certain tags in your dataset, same as in your webui prompts. <p>\n",
        "#@markdown Normal parentheses in your tags such as `(series names)` will need to be escaped like `\\(series names\\)`\n",
        "weighted_captions = False #@param {type:\"boolean\"}\n",
        "\n",
        "#markdown By enabling `adjust_tags`, you will let this colab modify your tags before running to automatically adjust to `weighted_captions` being on or off. <p>\n",
        "#markdown Then, you may increase `activation_tag_weight` to improve how effective your activation tag is.\n",
        "adjust_tags = False #param {type:\"boolean\"}\n",
        "activation_tag_weight = \"1.0\" #param [\"1.0\",\"1.1\",\"1.2\"]\n",
        "keep_tokens_weight = float(activation_tag_weight)\n",
        "\n",
        "#@markdown Here you can write a path in your Google Drive to load an existing Lora file to continue training on.<p>\n",
        "#@markdown **Warning:** It's not the same as one long training session. The epochs start from scratch, and it may have worse results.\n",
        "continue_from_lora = \"\" #@param {type:\"string\"}\n",
        "if continue_from_lora and not continue_from_lora.startswith(\"/content/drive/MyDrive\"):\n",
        "  import os\n",
        "  continue_from_lora = os.path.join(\"/content/drive/MyDrive\", continue_from_lora)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WDjkp4scvPgE"
      },
      "outputs": [],
      "source": [
        "#@markdown ### ğŸ“‚ Extraer datos\n",
        "#@markdown Es lento subir muchos archivos pequeÃ±os, si quieres puedes subir un zip y extraerlo aquÃ­.\n",
        "zip = \"/content/drive/MyDrive/mi_dataset.zip\" #@param {type:\"string\"}\n",
        "extract_to = \"/content/drive/MyDrive/Loras/ejemplo/dataset\" #@param {type:\"string\"}\n",
        "\n",
        "import os, zipfile\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "  from google.colab import drive\n",
        "  print(\"ğŸ“‚ Conectando a Google Drive...\")\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "os.makedirs(extract_to, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip, 'r') as f:\n",
        "  f.extractall(extract_to)\n",
        "\n",
        "print(\"âœ… Listo\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aKWlpsG0jrX3"
      },
      "outputs": [],
      "source": [
        "#@markdown ### ğŸ”¢ Contar archivos\n",
        "#@markdown Google Drive hace imposible contar los archivos en una carpeta, por lo que aquÃ­ puedes ver la cantidad de archivos en carpetas y subcarpetas.\n",
        "folder = \"/content/drive/MyDrive/Loras\" #@param {type:\"string\"}\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "    print(\"ğŸ“‚ Conectando a Google Drive...\\n\")\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "tree = {}\n",
        "exclude = (\"_logs\", \"/output\")\n",
        "for i, (root, dirs, files) in enumerate(os.walk(folder, topdown=True)):\n",
        "  dirs[:] = [d for d in dirs if all(ex not in d for ex in exclude)]\n",
        "  images = len([f for f in files if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))])\n",
        "  captions = len([f for f in files if f.lower().endswith(\".txt\")])\n",
        "  others = len(files) - images - captions\n",
        "  path = root[folder.rfind(\"/\")+1:]\n",
        "  tree[path] = None if not images else f\"{images:>4} images | {captions:>4} captions |\"\n",
        "  if tree[path] and others:\n",
        "    tree[path] += f\" {others:>4} other files\"\n",
        "\n",
        "pad = max(len(k) for k in tree)\n",
        "print(\"\\n\".join(f\"ğŸ“{k.ljust(pad)} | {v}\" for k, v in tree.items() if v))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrkIHHTOYfab"
      },
      "source": [
        "# ğŸ“ˆ Graficar resultados de entrenamiento\n",
        "Puedes hacer esto tras el entrenamiento. No es necesario a menos que sepas lo que haces.  \n",
        "Puede que la primera celda falle en cargar todos tus datos. Sigue intentando la segunda celda hasta que terminen de cargar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCNSq1kLYfab"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir={log_folder}/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPZPCy9xYfab"
      },
      "outputs": [],
      "source": [
        "from tensorboard import notebook\n",
        "notebook.display(port=6006, height=800)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}